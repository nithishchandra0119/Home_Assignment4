# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjKLn7fakhTgXY4_XDCxABLNImX3q6Qd

# Q1: NLP Preprocessing Pipeline
"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt_tab')


def preprocess_sentence(sentence):
    """
    Performs basic NLP preprocessing on a sentence.

    Args:
        sentence (str): The input sentence.

    Returns:
        None: Prints the original tokens, tokens without stopwords, and stemmed words.
    """
    # 1. Tokenize the sentence
    tokens = word_tokenize(sentence.lower())  # Convert to lowercase for consistency
    print("1. Original Tokens –", tokens)

    # 2. Remove common English stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [w for w in tokens if not w in stop_words]
    print("2. Tokens Without Stopwords –", filtered_tokens)

    # 3. Apply stemming
    porter = PorterStemmer()
    stemmed_words = [porter.stem(word) for word in filtered_tokens]
    print("3. Stemmed Words –", stemmed_words)

# Download necessary NLTK data if you haven't already
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    word_tokenize("example")
except LookupError:
    nltk.download('punkt')

# Example usage with the given sentence
sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."
preprocess_sentence(sentence)

"""# Q2: Named Entity Recognition with SpaCy"""

import spacy

def extract_named_entities(sentence):
    """
    Extracts named entities from a sentence using spaCy.

    Args:
        sentence (str): The input sentence.

    Returns:
        None: Prints the entity text, label, and start/end positions.
    """
    # Load the English language model
    nlp = spacy.load("en_core_web_sm")

    # Process the sentence
    doc = nlp(sentence)

    # Iterate through the entities and print their information
    for ent in doc.ents:
        print(f"Entity: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}")

# Input sentence
sentence = "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

# Extract and print named entities
extract_named_entities(sentence)

"""## Q3: Scaled Dot-Product Attention"""

import numpy as np
from scipy.special import softmax

def scaled_dot_product_attention(Q, K, V):
    """
    Implements the scaled dot-product attention mechanism.

    Args:
        Q (np.ndarray): Query matrix.
        K (np.ndarray): Key matrix.
        V (np.ndarray): Value matrix.

    Returns:
        tuple: A tuple containing the attention weights matrix and the output matrix.
    """
    # 1. Compute the dot product of Q and K transpose
    matmul_qk = np.matmul(Q, K.T)

    # 2. Scale the result by dividing by sqrt(dk)
    dk = K.shape[-1]
    scaled_attention_logits = matmul_qk / np.sqrt(dk)

    # 3. Apply softmax to get attention weights
    attention_weights = softmax(scaled_attention_logits, axis=-1)

    # 4. Multiply the weights by V to get the output
    output = np.matmul(attention_weights, V)

    return attention_weights, output

# Test inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

# Calculate attention weights and output
attention_weights, output = scaled_dot_product_attention(Q, K, V)

# Expected Output
print("1. Attention Weights Matrix:")
print(attention_weights)
print("\n2. Output Matrix:")
print(output)

"""# Q4: Sentiment Analysis using HuggingFace Transformers"""

from transformers import pipeline

def analyze_sentiment(sentence):
    """
    Analyzes the sentiment of a sentence using HuggingFace Transformers.

    Args:
        sentence (str): The input sentence.

    Returns:
        None: Prints the sentiment label and confidence score.
    """
    # Load the pre-trained sentiment analysis pipeline
    sentiment_pipeline = pipeline("sentiment-analysis")

    # Analyze the sentence
    result = sentiment_pipeline(sentence)[0]

    # Print the results
    print(f"Sentiment: {result['label']}")
    print(f"Confidence Score: {result['score']:.4f}")

# Input sentence
sentence = "Despite the high price, the performance of the new MacBook is outstanding."

# Analyze the sentiment
analyze_sentiment(sentence)